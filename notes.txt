
Remember that matlab is column major

In Train, train is initially n_training_sentences x max_sentence_length
- One row per sentence (padded out to max length)
- Entries are indexes into vocabulary

The different masks are used to make computing vectorized cost function easier (not sure exactly how used)

indices:
- indexes into words in a batch (?)
- is:
block of 1's - block of 2's - etc - block of word embedding size
each block has length batch_size * sentence_length + 1

A batch starts out as a matrix of indexes formatted as
n_sentences x max_sentence_length
and is trainsformed with: reshape(batch', 1, [])
This unwinds the batch into a row vector that is formatted as
words in first sentence - words in second sentence - etc
These are still indexes so this is NOT the same shape as the variable indices
this ends up in "minibatch"

NOW INSIDE COST FUNCTION



CR_E is n_dim x n_words_in_vocabulary, it contains one embedding vector for each word in the vocabulary

CR_E(:,minibatch) uses the indexes defined above (in minibatch) to extract the word embeddings
This matrix has one word embedding per column, as with minibatch it is
first words - second words - third words - etc
sentences are interleved in blocks

The first thing cost funtion does is a horrible transformation to this CR_E(:,minibatch).

The result is a 2d matrix formatted blockwise.
- The horizontal dimension is words in the sentence.
- The virtual dimension has the following nested blocks
 - minibatch_size -> n_feature_maps -> embedding_dim

This is data so it doesn't change for different feature maps, the blocks for different feature maps are identical
within a sentence.  Convolutions over words happen horizontally over this matrix.

Recall how feature maps are stored in matrices with the shape: n_feature_maps * embedding_dim x kernel_width
This means the feature map matrix can be replicated vertically minibatch_size times and is now has the same vertical
size as the molested data.

Here is a disection of the operation:

-----------------------------------

%%

clear;
clc;

embedding_dim = 2;
sentence_len = 5;
minibatch_size = 4;
n_feature_maps = 2;

CR_E = 0.1*kron(1:minibatch_size, ones(embedding_dim,sentence_len)) ...
    + 0.01*kron(ones(embedding_dim,minibatch_size), 1:sentence_len) ...
    + 0.001*repmat((1:embedding_dim)', 1, minibatch_size * sentence_len);

% values are 0.abc
% a = sentence
% b = word
% c = embedding dimension
%
% so the value 0.abc is the c'th dimension of the embedding of the b'th
% word in the ath sentence

% from CostFunction:
% data = reshape(permute(reshape(repmat(CR_E(:,minibatch),p(3),1),p(1)*p(3),p(2),size_mini),[1,3,2]),p(3)*p(1)*size_mini,p(2));
%
% p(1) = embedding_dim
% p(2) = sentence_len
% p(3) = n_feature_maps
% size_mini = minibatch_size
% CR_E = CR_E(:, minibatch)

% CR_E

% replicate vertically, once for each feature map
s1 = repmat(CR_E,n_feature_maps,1);
% s1


s2 = reshape(s1, [embedding_dim * n_feature_maps, sentence_len, minibatch_size]);
% s2

s3 = permute(s2, [1,3,2]);
% s3

s4 = reshape(s3, [minibatch_size * n_feature_maps * embedding_dim, sentence_len]);
s4

disp('------')

a = reshape(permute(reshape(s4', [],embedding_dim,n_feature_maps*minibatch_size),[2,1,3]), embedding_dim, []);
b = reshape(permute(reshape(s4', [],embedding_dim,n_feature_maps,minibatch_size),[2,1,3,4]), embedding_dim, []);

% reshape to break dimensions horizontally, order = smallest to largest nested blocks
% permute to bring a dimension to the front
% reshape to flatten horizontally, order = first dimension makes smallest
% blocks, second dim makes next bigger blocks and so on
c = reshape( ...
    permute( ...
        reshape( ...
            permute(s4, [2,1]), ...
            sentence_len, embedding_dim, n_feature_maps, minibatch_size), ...
        [2,1,3,4]), ... embedding_dim, sentence_len, n_feature_maps, minibatch_size
    embedding_dim, []);

---------------

As a general rule for reshuffling 2d representations.
- Permute to put the non blocked dimension first
- Reshape into n-d to recover blocks as dimensions
    - Order of blocks puts smallest blocks (fastest moving dim) on the left, largest blocks on the right
- Permute the n-d representation to put the new non-blocked dimension first
- Flatten the remaining dimensions to get a 2d representation.

Note: If consecutive dimensions in the n-d representation aren't going to have their order changed you can leave them flattened

M_1 (presumably "map 1") is the result of xcorrolating the first layer kernels with the molested data (fliplr because
the convollution operation undoes this internally)

M_1 is reshaped to put the embedding dimension first, and then folded.

The "depth" of a feature map is the size of the feature map in the non-convolutional direction, after folding

